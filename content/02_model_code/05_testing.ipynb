{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e690c549",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "This notebook provides a set of tests to run against `treat_sim`.  These tests are either pass or fail and no interpretation is needed. A summary of test results is provided at the end of the notebook.\n",
    "\n",
    "> This notebook is a work in progress.\n",
    "\n",
    "We have broken the testing into the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca20860",
   "metadata": {},
   "source": [
    "## 1. Model Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c3cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from treat_sim.model import (\n",
    "    Scenario, \n",
    "    TreatmentCentreModel,\n",
    "    single_run, \n",
    "    multiple_replications\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48018d85",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ba1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13604fb2",
   "metadata": {},
   "source": [
    "## 3. Tests\n",
    "\n",
    "### 3.1 Model run test\n",
    "\n",
    "Here we test that various modes of running the model work correctly.  These include\n",
    "\n",
    "* single run mode\n",
    "* repeatable results using random number sets.\n",
    "* results collection period\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7d4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_run_type():\n",
    "    '''\n",
    "    Test a single_run of the model.\n",
    "    \n",
    "    The single_run function should return a pandas.DataFrame\n",
    "    containing 16 columns and a single row.\n",
    "    \n",
    "     0   00_arrivals                    \n",
    "     1   01a_triage_wait                 \n",
    "     2   01b_triage_util               \n",
    "     3   02a_registration_wait         \n",
    "     4   02b_registration_util        \n",
    "     5   03a_examination_wait          \n",
    "     6   03b_examination_util          \n",
    "     7   04a_treatment_wait(non_trauma)  \n",
    "     8   04b_treatment_util(non_trauma)  \n",
    "     9   05_total_time(non-trauma)       \n",
    "     10  06a_trauma_wait               \n",
    "     11  06b_trauma_util                 \n",
    "     12  07a_treatment_wait(trauma)      \n",
    "     13  07b_treatment_util(trauma)      \n",
    "     14  08_total_time(trauma)           \n",
    "     15  09_throughput                   \n",
    "\n",
    "    Expected result: \n",
    "    ---------------\n",
    "        len(run_results) == 16 and isinstance(run_results, pd.Dataframe)\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    bool: does the model pass the test.\n",
    "    '''\n",
    "    EXPECTED_LENGTH = 16\n",
    "\n",
    "    # a default experiment\n",
    "    default_experiment_params = Scenario(random_number_set=41)\n",
    "\n",
    "    # run the model in single run model\n",
    "    run_results = single_run(default_experiment_params)\n",
    "\n",
    "    # test\n",
    "    assert len(run_results.T) == EXPECTED_LENGTH and isinstance(run_results, pd.DataFrame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc98fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize('random_number_set', [\n",
    "                          (0),\n",
    "                          (1),\n",
    "                          (2),\n",
    "                          (101),\n",
    "                          (42),\n",
    "])\n",
    "def test_random_number_set(random_number_set):\n",
    "    '''\n",
    "    Test the model produces repeatable results\n",
    "    given the same set set of random seeds.\n",
    "    \n",
    "    Expected result: \n",
    "    ---------------\n",
    "        difference between data frames is 0.0\n",
    "    '''\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(2):\n",
    "        \n",
    "        exp = Scenario()\n",
    "\n",
    "        # run the model in single run model\n",
    "        run_results = single_run(exp, random_no_set=random_number_set)\n",
    "    \n",
    "        results.append(run_results)\n",
    "        \n",
    "    # test\n",
    "    assert (results[0] - results[1]).sum().sum() == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa609eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize('rc_period', [\n",
    "                          (10.0),\n",
    "                          (1_000.0),\n",
    "                          (25.0),\n",
    "                          (500.0),\n",
    "                          (143.0),\n",
    "])\n",
    "def test_run_length_control(rc_period):\n",
    "    scenario = Scenario()\n",
    "    \n",
    "    # set random number set - this controls sampling for the run.\n",
    "    scenario.set_random_no_set(42)\n",
    "\n",
    "    # create an instance of the model\n",
    "    model = TreatmentCentreModel(scenario)\n",
    "\n",
    "    # run the model\n",
    "    model.run(results_collection_period=rc_period)\n",
    "        \n",
    "    # run results\n",
    "    assert model.env.now == rc_period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62af16f",
   "metadata": {},
   "source": [
    "## Run all automated tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f94807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 11 items\n",
      "\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_single_run_type \u001b[32mPASSED\u001b[0m\u001b[32m                           [  9%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_random_number_set[0] \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 18%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_random_number_set[1] \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 27%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_random_number_set[2] \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 36%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_random_number_set[101] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 45%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_random_number_set[42] \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 54%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_run_length_control[10.0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 63%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_run_length_control[1000.0] \u001b[32mPASSED\u001b[0m\u001b[32m                [ 72%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_run_length_control[25.0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 81%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_run_length_control[500.0] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 90%]\u001b[0m\n",
      "t_e51d2d1d981b4a758ee6ead7bc5df832.py::test_run_length_control[143.0] \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m11 passed\u001b[0m\u001b[32m in 2.05s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.run(\"-vv\", \"--no-header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aec4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92d9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
